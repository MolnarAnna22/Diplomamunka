{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Felesleges változóktól megszűrt kommentek betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/ usr/bin/env python\n",
    "import subprocess\n",
    "from os. path import isfile , join\n",
    "in_path = \"C:/Users/THINKPAD/Documents/\"\n",
    "out_path = \"C:/Users/THINKPAD/OneDrive/Dokumentumok/outfiles/\"\n",
    "# running magyarlanc on raw corpora\n",
    "\n",
    "#i = 0\n",
    "#while i <= 2192772:\n",
    "#    f = str(i) + '.txt '\n",
    "#    if isfile(join(in_path, f)):\n",
    "#        subprocess.call(\"java -Xmx1G -jar magyarlanc-3.0.jar -mode morphparse -input \" + join(in_path , f) + \" -output \" + join(out_path , f), shell = True )\n",
    "#    i += 1\n",
    "print('DONE ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('covid1.csv', sep = \",\", encoding = 'utf8', error_bad_lines=False, quotechar='\"')\n",
    "df2 = pd.read_csv('covid2.csv', sep = \",\", encoding = 'utf8', error_bad_lines=False, quotechar='\"')\n",
    "df3 = pd.read_csv('covid3.csv', sep = \",\", encoding = 'utf8', error_bad_lines=False, quotechar='\"')\n",
    "df4 = pd.read_csv('covid4.csv', sep = \",\", encoding = 'utf8', error_bad_lines=False, quotechar='\"')\n",
    "df5 = pd.read_csv('covid5.csv', sep = \",\", encoding = 'utf8', error_bad_lines=False, quotechar='\"')\n",
    "df6 = pd.read_csv('covid6.csv', sep = \",\", encoding = 'utf8', error_bad_lines=False, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = df1.append(df2, ignore_index=True)\n",
    "df123 = df12.append(df3, ignore_index=True)\n",
    "df1234 = df123.append(df4, ignore_index=True)\n",
    "df12345 = df1234.append(df5, ignore_index=True)\n",
    "df_ossz = df12345.append(df6, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ossz.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adat = {}\n",
    "for i in range(len(df_ossz.id)):\n",
    "    adat[df_ossz.id[i]] = df_ossz[\"Content of posts\"][i]\n",
    "print(list(adat.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_multidict = {}\n",
    "for key, value in adat.items():\n",
    "    rev_multidict.setdefault(value, set()).add(key)\n",
    "\n",
    "#[key for key, values in rev_multidict.items() if len(values) > 1]\n",
    "toremove_all = [list(values) for key, values in rev_multidict.items() if len(values) > 1] # így bekerül a listába az összes ismétlődés, de abból az egyiket szeretnénk megtartani\n",
    "for i in range(len(toremove_all)):\n",
    "    del toremove_all[i][-1] #így mindig kiszedünk a duplikációk közül egyet, ami bent marad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toremove_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ossz.index[df_ossz.id == '3394960137220799_3395988493784630']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ossz[\"Content of posts\"][241208]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ossz.index[df_ossz.id == '10158342543064346_10158346069524346']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ossz[\"Content of posts\"][303]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(toremove_all) #összesen ennyiféle egyező van"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toremove = []\n",
    "for i in range(len(toremove_all)):\n",
    "    for j in range(len(toremove_all[i])):\n",
    "        toremove.append(toremove_all[i][j])\n",
    "len(toremove) ## ennyi kommentet lehet törölni dublikáció miatt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupl = df_ossz[~df_ossz[\"id\"].isin(toremove)]\n",
    "df_nodupl = df_nodupl.reset_index(drop=True)\n",
    "df_nodupl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_length = []\n",
    "for i in range(len(df_nodupl[\"Content of posts\"])):\n",
    "    txt_length.append(len(str(df_nodupl[\"Content of posts\"][i]).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egyszo = 0\n",
    "for i in txt_length:\n",
    "    if i == 1:\n",
    "        egyszo +=1\n",
    "print(egyszo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sokszo = 0\n",
    "for i in txt_length:\n",
    "    if i > 100:\n",
    "        sokszo += 1\n",
    "print(sokszo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kevesszo = 0\n",
    "for i in txt_length:\n",
    "    if i < 5:\n",
    "        kevesszo +=1\n",
    "print(kevesszo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 4 szónál hosszabb és maximum 100 szavas kommentek betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('del_length (1).csv', sep = \",\", encoding = 'utf8', error_bad_lines=False, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = df[\"Created\"].str[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"date\"] == \"2020-01\"][\"Content of posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2020.január: \" + str(df[df[\"date\"] == \"2020-01\"].shape))\n",
    "print(\"2020.február: \" + str(df[df[\"date\"] == \"2020-02\"].shape))\n",
    "print(\"2020.március: \" + str(df[df[\"date\"] == \"2020-03\"].shape))\n",
    "print(\"2020.április: \" + str(df[df[\"date\"] == \"2020-04\"].shape))\n",
    "print(\"2020.május: \" + str(df[df[\"date\"] == \"2020-05\"].shape))\n",
    "print(\"2020.június: \" + str(df[df[\"date\"] == \"2020-06\"].shape))\n",
    "print(\"2020.július: \" + str(df[df[\"date\"] == \"2020-07\"].shape))\n",
    "print(\"2020.augusztus: \" + str(df[df[\"date\"] == \"2020-08\"].shape))\n",
    "print(\"2020.szeptember: \" + str(df[df[\"date\"] == \"2020-09\"].shape))\n",
    "print(\"2020.október: \" + str(df[df[\"date\"] == \"2020-10\"].shape))\n",
    "print(\"2020.november: \" + str(df[df[\"date\"] == \"2020-11\"].shape))\n",
    "print(\"2020.december: \" + str(df[df[\"date\"] == \"2020-12\"].shape))\n",
    "print(\"2021.január: \" + str(df[df[\"date\"] == \"2021-01\"].shape))\n",
    "print(\"2021.február: \" + str(df[df[\"date\"] == \"2021-02\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = [\"2020.január\", \"2020.február\", \"2020.március\", \"2020.április\", \"2020.május\", \"2020.június\", \"2020.július\",\n",
    "            \"2020.augusztus\", \"2020.szeptember\", \"2020.október\", \"2020.november\", \"2020.december\", \"2021.január\", \"2021.február\"]\n",
    "number_list = [10789, 8239, 22974, 28915, 25678, 19579, 19314, 46253, 43888, 80849, 237965, 500788, 960577, 186965]\n",
    "date_202010 = [\"2020.január\", \"2020.február\", \"2020.március\", \"2020.április\", \"2020.május\", \"2020.június\", \"2020.július\",\n",
    "            \"2020.augusztus\", \"2020.szeptember\", \"2020.október\"]\n",
    "number_202010 = [10789, 8239, 22974, 28915, 25678, 19579, 19314, 46253, 43888, 80849]\n",
    "date_202102 = [\"2020.november\", \"2020.december\", \"2021.január\", \"2021.február\"]\n",
    "number_202102 = [237965, 500788, 960577, 186965]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "degrees = 70\n",
    "plt.xticks(rotation=degrees)\n",
    "plt.plot(date_list, number_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = 70\n",
    "plt.xticks(rotation=degrees)\n",
    "plt.plot(date_202010, number_202010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = 70\n",
    "plt.xticks(rotation=degrees)\n",
    "plt.plot(date_202102, number_202102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_202002_202101 = df[(df[\"date\"] != \"2020-01\") & (df[\"date\"] != \"2021-02\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_202002_202101 = df_202002_202101.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(df_202002_202101)):\n",
    "    text_file = open(in_path + str(j) + \".txt\", \"w\", encoding = \"utf8\")\n",
    "    text_file.write(df_202002_202101[\"text_nopunct\"][j])\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(in_path + \"covid_szavak\" + \".txt\", \"w\", encoding = \"utf8\")\n",
    "\n",
    "for i in range(20000):\n",
    "    text_file.write(df_202002_202101[\"text_nopunct\"][i] + \" @@@ \")\n",
    "    \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_fwf(in_path + \"covid.txt\", delimiter = \" @@@ \", encoding = \"utf8\", header = None)\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Különleges karakterek törlése "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \"'^&*_+<>?:„;\"\n",
    "df[\"Content of posts\"] = [''.join(c for c in s if c not in punctuation) for s in df[\"Content of posts\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Content of posts\"][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szövegben \\n-ek kicserélése sima szóközre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df.replace(r'\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[\"Content of posts\"][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dupla space kicserélése egyre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.replace('  ', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[\"Content of posts\"][1][137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[\"Content of posts\"][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "accent = ['á', 'é', 'í', 'ó', 'ö', 'ő', 'ú', 'ü', 'ű']\n",
    "\n",
    "accented = {}\n",
    "for i in range(len(df[\"Content of posts\"])):\n",
    "    accented[str(i)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in accent:\n",
    "    for i in range(len(df[\"Content of posts\"])):\n",
    "        if j in df[\"Content of posts\"][i]:\n",
    "            accented[str(i)] += df[\"Content of posts\"][i].count(j)\n",
    "        else:\n",
    "            accented[str(i)] += 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(accented.values())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(list(accented.values())).keys()) # equals to list(set(words))\n",
    "print(Counter(list(accented.values())).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(list(accented.values()), ax=ax, hist=True, kde=False, \n",
    "             bins=range(31), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "ax.set_xlim(0, 31)\n",
    "ax.set_xticks(range(31))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulla = []\n",
    "for (key, value) in accented.items():\n",
    "    if value == 0:\n",
    "        nulla.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nulla)):\n",
    "    nulla[i] = int(nulla[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hossz = []\n",
    "for i in nulla:\n",
    "    hossz.append(table[\"txt_length\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "statistics.mean(hossz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.stdev(hossz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.distplot(hossz, ax=ax, hist=True, kde=False, \n",
    "             bins=range(31), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "ax.set_xlim(5, 31)\n",
    "ax.set_xticks(range(5, 31))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egy = []\n",
    "for (key, value) in accented.items():\n",
    "    if value == 1:\n",
    "        egy.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(egy)):\n",
    "    egy[i] = int(egy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hossz_egy = []\n",
    "for i in egy:\n",
    "    hossz_egy.append(table[\"txt_length\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(hossz_egy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.stdev(hossz_egy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.distplot(hossz_egy, ax=ax, hist=True, kde=False, \n",
    "             bins=range(31), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "ax.set_xlim(5, 31)\n",
    "ax.set_xticks(range(5, 31))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizálás, lemmatizálás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_202002_202101.to_csv(\"df_202002_202101.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hu_core_ud_lg\n",
    "\n",
    "nlp = hu_core_ud_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Csak a melléknevek, igék, főnevek, tulajdonnevek, határozószavak, számok maradnak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ezenkívül töröltem azokat a szavakat, amik benne vannak a stoplistában és amik nem betűkből állnak (pl. számok, emojik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = ['ADV', 'VERB', 'NOUN', 'ADJ', 'NUM', 'PROPN']\n",
    "\n",
    "kommentek = []\n",
    "for txt in table[\"Content of posts\"]:\n",
    "    doc = nlp(txt)\n",
    "    proc = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha and token.pos_ in allowed and not token.is_stop:\n",
    "            proc.append(str(token.lemma_).lower())\n",
    "    txt = ' '.join(proc)\n",
    "    kommentek.append(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ezeket a lemmatizált kommenteket is hozzávettem a dataframe-hez és kimentettem egy új fájlba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_token = pd.read_csv(\"tokenized_comments_noverb.csv\", sep = \",\", encoding = 'utf8', error_bad_lines=False, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_token.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ékezetes karakterek aránya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "accent = ['á', 'é', 'í', 'ó', 'ö', 'ő', 'ú', 'ü', 'ű']\n",
    "\n",
    "accented = {}\n",
    "for i in range(len(table_token[\"kommentek\"])):\n",
    "    accented[str(i)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Szótárba kiszedtem, hogy melyik kommentben hány ékezetes betű található"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(table_token[\"kommentek\"])):\n",
    "    for j in accent:\n",
    "        if j in str(table_token[\"kommentek\"][i]):\n",
    "            accented[str(i)] += 1\n",
    "        else:\n",
    "            accented[str(i)] += 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illetve megnéztem azt is, hogy maximálisan hány ékezet szerepelt a szövegekben, illetve az egyes ékezetmennyiségnek milyen volt az előfordulása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(list(accented.values())).keys()) # equals to list(set(words))\n",
    "print(Counter(list(accented.values())).values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "281760 szövegben egyáltalán nem volt ékezet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(list(accented.values()), ax=ax, hist=True, kde=False, \n",
    "             bins=range(11), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "ax.set_xlim(0, 11)\n",
    "ax.set_xticks(range(11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Töröljük azokat, ahol üres a komment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_new = table_token.drop(table_token[table_token[\"kommentek\"].isna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_new = table_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "txt = list(table_new[\"kommentek\"])\n",
    "\n",
    "txts = []\n",
    "for t in txt:\n",
    "    txts.append(t.strip().split())\n",
    "words_sigbig = list(itertools.chain(*txts))\n",
    "\n",
    "b = (pd.Series(nltk.ngrams(words_sigbig, 2)).value_counts())[:1000]\n",
    "bigrams = b.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szignifikáns bigramok keresése és cseréje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of = codecs.open('significant_bigrams_2021_covid.tsv', 'w', 'utf-8')\n",
    "for e in bigrams:\n",
    "    exp = e[0] + ' ' + e[1] + \"\\t\" + e[0] + \"_\" + e[1] + '\\n'\n",
    "    of.write(exp)\n",
    "of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = codecs.open('significant_bigrams_2021_covid.tsv', 'r', 'utf-8') # a szerkesztett verziót hívd be!!\n",
    "\n",
    "bigrams = []\n",
    "for l in f:\n",
    "    w1, w2 = l.strip().split('\\t')\n",
    "    tup = w1,w2\n",
    "    bigrams.append(tup)\n",
    "\n",
    "def filter_bigrams(text):\n",
    "    text = \" \" + text + \" \"\n",
    "    for i in bigrams:\n",
    "        text = text.replace(\" \" + i[0] + \" \", \" \" + i[1] + \" \" )\n",
    "    return text\n",
    "\n",
    "sigbigrams = []\n",
    "for komment in list(table_new[\"kommentek\"]):\n",
    "    sigbigrams.append(filter_bigrams(komment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigbigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hibásan lemmatizált szavak javítása kézzel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sigbigrams = []\n",
    "\n",
    "for sentence in sigbigrams:\n",
    "    new_string = sentence.replace(\" covi_vírus \", \" covid_vírus \")\n",
    "    new_string2 = new_string.replace(\" covi \", \" covid \")\n",
    "    new_string3 = new_string2.replace(\" covi_osztály \", \" covid_osztály \")\n",
    "    new_string4 = new_string3.replace(\" covi_vakcina \", \" covid_vakcina \")\n",
    "    new_string5 = new_string4.replace(\" covi_oltás \", \" covid_oltás \")\n",
    "    new_string6 = new_string5.replace(\" covi_elleni \", \" covid_elleni \")\n",
    "    new_string7 = new_string6.replace(\" ast_zeneca \", \" astrazeneca \")\n",
    "    new_string8 = new_string7.replace(\" orosz_vakcinat \", \" orosz_vakcinat \")\n",
    "    new_string9 = new_string8.strip()\n",
    "    \n",
    "    new_sigbigrams.append(new_string9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szógyakoriság alapú szűrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "txts = []\n",
    "\n",
    "for text in new_sigbigrams:\n",
    "    txt = text.strip().split()\n",
    "    txt = [w for w in txt if len(w) > 3]\n",
    "    txts.append(txt)\n",
    "\n",
    "        \n",
    "wordlist = list(itertools.chain(*txts))\n",
    "wfreq = Counter(wordlist)\n",
    "sorted_wfreq = sorted(wfreq.items(),\n",
    "                      key=operator.itemgetter(1),\n",
    "                      reverse=True)\n",
    "\n",
    "with codecs.open('wfreq.tsv', 'w', 'utf-8') as f:\n",
    "    for e in sorted_wfreq:\n",
    "        o = e[0] + '\\t' + str(e[1]) + '\\n'\n",
    "        f.write(o)\n",
    "\n",
    "wd_docfreq = {}\n",
    "for d in txts:\n",
    "    d = list(set(d))\n",
    "    for w in d:\n",
    "        if w not in wd_docfreq.keys():\n",
    "            wd_docfreq[w] = 1\n",
    "        else:\n",
    "            wd_docfreq[w] += 1\n",
    "\n",
    "sorted_docfreq = sorted(wd_docfreq.items(),\n",
    "                        key=operator.itemgetter(1),\n",
    "                        reverse=True)\n",
    "\n",
    "with codecs.open('docfreq.tsv', 'w', 'utf-8') as f:\n",
    "    for e in sorted_docfreq:\n",
    "        o = e[0] + '\\t' + str((e[1]/len(txts))) + '\\n'\n",
    "        f.write(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv('stopszavak.txt', delimiter = \"\\t\", header = None, names = [\"words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stopszavakat a 4000 leggyakoribb szó átnézésével válogattam ki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Több alakú, de azonos jelentésű bigramok egységesítése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sigbigrams2 = []\n",
    "for text in new_sigbigrams:\n",
    "    for word in list(stopwords[\"words\"]):\n",
    "        text = text.replace(word + \" \", '')\n",
    "    text = text.replace(\"pfizer_vakcina \", \"pfizer \")\n",
    "    text = text.replace(\"pfizer_oltás \", \"pfizer \")\n",
    "    text = text.replace(\"pfizer_biontech \", \"pfizer \")\n",
    "    text = text.replace(\"astrazenec \", \"astrazeneca \")\n",
    "    text = text.replace(\"moderna_vakcina \", \"moderna \")\n",
    "    text = text.replace(\"szputnyik \", \"orosz_vakcina \")\n",
    "    text = text.replace(\"szputnyik_v \", \"orosz_vakcina \")\n",
    "    text = text.replace(\"orosz_oltás \", \"orosz_vakcina \")\n",
    "    text = text.replace(\"orosz_oltóanyag \", \"orosz_vakcina \")\n",
    "    text = text.replace(\"ruszki_vakcina \", \"orosz_vakcina \")\n",
    "    text = text.replace(\"kinai_vakcina \", \"kínai_vakcina \")\n",
    "    text = text.replace(\"kínai_oltás \", \"kínai_vakcina \")\n",
    "    text = text.replace(\"kínai_oltóanyag \", \"kínai_vakcina \")\n",
    "    text = text.replace(\"sinopharm \", \"kínai_vakcina \")\n",
    "    text = text.replace(\"orbán \", \"orbán_viktor \")\n",
    "    text = text.replace(\"vitya \", \"orbán_viktor \")\n",
    "    text = text.replace(\"miniszterelnök \", \"orbán_viktor \")\n",
    "    text = text.replace(\"zorbán \", \"orbán_viktor \")\n",
    "    text = text.replace(\"szijjártó \", \"szijjártó_péter \")\n",
    "    text = text.replace(\"szíjjártó \", \"szijjártó_péter \")\n",
    "    text = text.replace(\"szijártó \", \"szijjártó_péter \")\n",
    "    text = text.replace(\"szíjártó \", \"szijjártó_péter \")\n",
    "    text = text.replace(\"külügyminiszter \", \"szijjártó_péter \")\n",
    "    text = text.replace(\"cecilia \", \"müller_cecília \")\n",
    "    text = text.replace(\"cecília \", \"müller_cecília \")\n",
    "    text = text.replace(\"ceci \", \"müller_cecília \")\n",
    "    text = text.replace(\"cilike \", \"müller_cecília \")\n",
    "    text = text.replace(\"müller \", \"müller_cecília \")\n",
    "    text = text.replace(\"bill \", \"bill_gates \")\n",
    "    text = text.replace(\" gates \", \"bill_gates \")\n",
    "    text = text.replace(\"gődény \", \"gődény_györgy \")\n",
    "    text = text.replace(\"gödény \", \"gődény_györgy \")\n",
    "    text = text.replace(\"szlávik \", \"szlávik_jános \")\n",
    "    text = text.replace(\"szlávik_doktor \", \"szlávik_jános \")\n",
    "    new_sigbigrams2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_new[\"sigbigrams\"] = new_sigbigrams2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount = []\n",
    "for item in list(table_new[\"sigbigrams\"]):\n",
    "    wordcount.append(len(item.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_new[\"wordcount\"] = wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-nál kevesebb szót tartalmazó kommentek törlése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_new2 = table_new[~(table_new[\"wordcount\"] < 3)]\n",
    "table_new2.drop(columns = [\"wordcount\"], inplace = True)\n",
    "table_new2 = table_new2.reset_index(drop=True)\n",
    "table_new2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "txts2 = []\n",
    "\n",
    "for text in list(table_new2[\"sigbigrams\"]):\n",
    "    txt = text.strip().split()\n",
    "    txt = [w for w in txt if len(w) > 3]\n",
    "    txts2.append(txt)\n",
    "\n",
    "        \n",
    "wordlist = list(itertools.chain(*txts2))\n",
    "wfreq = Counter(wordlist)\n",
    "sorted_wfreq = sorted(wfreq.items(),\n",
    "                      key=operator.itemgetter(1),\n",
    "                      reverse=True)\n",
    "\n",
    "with codecs.open('wfreq2.tsv', 'w', 'utf-8') as f:\n",
    "    for e in sorted_wfreq:\n",
    "        o = e[0] + '\\t' + str(e[1]) + '\\n'\n",
    "        f.write(o)\n",
    "\n",
    "wd_docfreq = {}\n",
    "for d in txts2:\n",
    "    d = list(set(d))\n",
    "    for w in d:\n",
    "        if w not in wd_docfreq.keys():\n",
    "            wd_docfreq[w] = 1\n",
    "        else:\n",
    "            wd_docfreq[w] += 1\n",
    "\n",
    "sorted_docfreq = sorted(wd_docfreq.items(),\n",
    "                        key=operator.itemgetter(1),\n",
    "                        reverse=True)\n",
    "\n",
    "with codecs.open('docfreq2.tsv', 'w', 'utf-8') as f:\n",
    "    for e in sorted_docfreq:\n",
    "        o = e[0] + '\\t' + str((e[1]/len(txts))) + '\\n'\n",
    "        f.write(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_words = []\n",
    "of = codecs.open('contentwords6.tsv', 'w', 'utf-8')\n",
    "with codecs.open('docfreq2.tsv', 'r', 'utf-8') as input_file:\n",
    "    for l in input_file:\n",
    "        wd, dfreq = l.strip().split('\\t')\n",
    "        dfreq = float(dfreq)\n",
    "        if dfreq > 0.00014:\n",
    "            content_words.append(wd)\n",
    "            o = wd + '\\t' + str(dfreq) + '\\n'\n",
    "            of.write(o)\n",
    "of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = pd.read_table(\"contentwords6.tsv\", header = None, names = [\"words\", \"freq\"])\n",
    "input_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_words = list(input_file[\"words\"])\n",
    "\n",
    "clear_text = []\n",
    "files = list(table_new2[\"sigbigrams\"])\n",
    "for f in files:\n",
    "    text = \"\"\n",
    "    txt = \"\"\n",
    "    txt = f.strip().split()\n",
    "    for cw in txt:\n",
    "        if cw in content_words:\n",
    "            text += cw + \" \"\n",
    "    clear_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_new2[\"clear_text\"] = clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_new3 = table_new2[~(table_new2[\"clear_text\"] == \"\")]\n",
    "table_new3 = table_new3.reset_index(drop = True)\n",
    "table_new3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_new3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_new3.to_csv(\"df_final0810.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv(\"df_final0810.csv\", sep = \",\", encoding = \"utf8\")\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Első körös LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pyLDAvis.gensim\n",
    "import stat\n",
    "from collections import defaultdict\n",
    "from gensim.corpora import MalletCorpus\n",
    "from gensim.models import CoherenceModel\n",
    "from os import listdir\n",
    "from os.path import join, isfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = list(final[\"clear_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = []\n",
    "for t in txt:\n",
    "    txts.append(t.strip().split())\n",
    "words = list(itertools.chain(*txts))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(txts)\n",
    "dictionary.save('dictionary.dict') # a dictionary mentése (figyelj az útvonalra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in txts]\n",
    "MalletCorpus.serialize(\"corpus.mallet\", corpus, dictionary) # a corpus mentése (figyelj az útvonalra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = \"C:/Users/THINKPAD/Downloads/mallet-2.0.8/mallet-2.0.8/bin/mallet\"\n",
    "os.environ.update({'MALLET_HOME': r'C:/Users/THINKPAD/Downloads/mallet-2.0.8/mallet-2.0.8/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='C:/Users/THINKPAD/Downloads/gensim.log', # útvonal!!!\n",
    "                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of = codecs.open('C:/Users/THINKPAD/Downloads/coherence_values3.txt', 'w', 'utf-8')\n",
    "for run_number in range(3):\n",
    "    rs = 1\n",
    "    for tn in range(5, 15):\n",
    "        np.random.seed(123)\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path,\n",
    "                                                 corpus=corpus,\n",
    "                                                 num_topics=tn,\n",
    "                                                 id2word=dictionary,\n",
    "                                                 random_seed=rs+run_number)\n",
    "        rs += 1\n",
    "        lda = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model)\n",
    "        lda.save(('C:/Users/THINKPAD/Downloads/lda_modellek3/lda_t' + str(tn) + '_r' + str(run_number) +'.model'), ignore=())\n",
    "\n",
    "        coherence_model = CoherenceModel(model=model,\n",
    "                                         texts=txts,\n",
    "                                         dictionary=dictionary,\n",
    "                                         coherence='c_v')\n",
    "        coherence_value = coherence_model.get_coherence()\n",
    "        exp =  'topic ' +str(tn) + ' run' + str(run_number) + ' c_v: ' + str(coherence_value) + '\\n'\n",
    "        of.write(exp)\n",
    "of.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel.load('C:/Users/THINKPAD/Downloads/lda_modellek3/lda_t14_r1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[corpus[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'C:/Users/THINKPAD/Downloads/lda_modellek3/vizu/lda_t14_r1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=model, corpus=corpus, texts=final[\"clear_text\"]):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=final[\"clear_text\"])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic[\"Original_text\"] = final[\"Content of posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dominant_topic[df_dominant_topic[\"Dominant_Topic\"] != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(\"dominant_topics_t14_r1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Első körös LDA cv értékei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pd.read_table(\"coherence_values3.txt\", sep = \" \", header = None, names = [\"T\", \"Topic_number\", \"Run\", \"Measure\", \"CV\"])\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure(figsize=(8, 5), dpi=80)\n",
    "cv.set_index('Topic_number', inplace=True)\n",
    "cv.groupby('Run')['CV'].plot(legend=True)\n",
    "plt.xlabel('Topikok száma')\n",
    "plt.ylabel('C_v érték')\n",
    "plt.savefig(\"cv_14.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic = pd.read_csv(\"dominant_topics_t14_r1.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulla = df_dominant_topic[df_dominant_topic[\"Dominant_Topic\"] == 13.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulla[nulla[\"Topic_Perc_Contrib\"] >= 0.180].reset_index(drop=True)[\"Original_text\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"merged_df.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Csak a 3 szavas vagy annál hosszabb kommentek megtartása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2 = final_df[final_df[\"Len\"] >= 3]\n",
    "final2 = final2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2.to_csv(\"final_df_new.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Második körös LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = list(final2[\"clear_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = []\n",
    "for t in txt:\n",
    "    txts.append(t.strip().split())\n",
    "words = list(itertools.chain(*txts))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(txts)\n",
    "dictionary.save('dictionary.dict') # a dictionary mentése (figyelj az útvonalra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in txts]\n",
    "MalletCorpus.serialize(\"corpus.mallet\", corpus, dictionary) # a corpus mentése (figyelj az útvonalra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of = codecs.open('C:/Users/THINKPAD/Downloads/coherence_values_uj.txt', 'w', 'utf-8')\n",
    "for run_number in range(3):\n",
    "    rs = 1\n",
    "    for tn in range(5, 42):\n",
    "        np.random.seed(123)\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path,\n",
    "                                                 corpus=corpus,\n",
    "                                                 num_topics=tn,\n",
    "                                                 id2word=dictionary,\n",
    "                                                 random_seed=rs+run_number)\n",
    "        rs += 1\n",
    "        lda = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model)\n",
    "        lda.save(('C:/Users/THINKPAD/Downloads/lda_modellek_uj/lda_t' + str(tn) + '_r' + str(run_number) +'.model'), ignore=())\n",
    "\n",
    "        coherence_model = CoherenceModel(model=model,\n",
    "                                         texts=txts,\n",
    "                                         dictionary=dictionary,\n",
    "                                         coherence='c_v')\n",
    "        coherence_value = coherence_model.get_coherence()\n",
    "        exp =  'topic ' +str(tn) + ' run' + str(run_number) + ' c_v: ' + str(coherence_value) + '\\n'\n",
    "        of.write(exp)\n",
    "of.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Koherencia értékek kirajzoltatása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pd.read_excel(\"coh_value.xlsx\")\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "cv.plot(x=\"Topik\", y=[\"Run0\", \"Run1\", \"Run2\"], grid = True)\n",
    "plt.xlabel('Topikok száma')\n",
    "plt.ylabel('C_v érték')\n",
    "plt.savefig(\"cv41_values.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel.load('C:/Users/THINKPAD/Downloads/lda_modellek_uj/lda_t35_r2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'C:/Users/THINKPAD/Downloads/lda_t35_r2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA aszimmetrikus paraméterekkel (próba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of = codecs.open('C:/Users/THINKPAD/Downloads/coherence_values_asym.txt', 'w', 'utf-8')\n",
    "for run_number in range(3):\n",
    "    rs = 1\n",
    "    for tn in range(5, 42):\n",
    "        np.random.seed(123)\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path,\n",
    "                                                 corpus=corpus,\n",
    "                                                 num_topics=tn,\n",
    "                                                 id2word=dictionary,\n",
    "                                                 optimize_interval = 10,\n",
    "                                                 random_seed=rs+run_number)\n",
    "        rs += 1\n",
    "        lda = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model)\n",
    "        lda.save(('C:/Users/THINKPAD/Downloads/lda_modellek_asym/lda_t' + str(tn) + '_r' + str(run_number) +'.model'), ignore=())\n",
    "\n",
    "        coherence_model = CoherenceModel(model=model,\n",
    "                                         texts=txts,\n",
    "                                         dictionary=dictionary,\n",
    "                                         coherence='c_v')\n",
    "        coherence_value = coherence_model.get_coherence()\n",
    "        exp =  'topic ' +str(tn) + ' run' + str(run_number) + ' c_v: ' + str(coherence_value) + '\\n'\n",
    "        of.write(exp)\n",
    "of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = pd.read_excel(\"cv_values_asym.xlsx\")\n",
    "cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "cv2.plot(x=\"Topik\", y=[\"Run0\", \"Run1\", \"Run2\"], grid = True)\n",
    "plt.xlabel('Topikok száma')\n",
    "plt.ylabel('C_v érték')\n",
    "plt.savefig(\"cv41_values_asym.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel.load('E:/Anna/lda_modellek7/lda_t41_r0.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis, 'C:/Users/THINKPAD/Downloads/lda_t41_r0.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.LdaModel.load('C:/Users/THINKPAD/Downloads/lda_modellek_asym/lda_t41_r1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis2 = pyLDAvis.gensim.prepare(model2, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(vis2, 'C:/Users/THINKPAD/Downloads/lda_asym_t41_r1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domináns topikba sorolás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=model, corpus=corpus, texts=final2[\"clear_text\"]):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=final2[\"clear_text\"])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic[\"Original_text\"] = final2[\"Content of posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic[\"Date\"] = final2[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(\"dominant_topics_850k_t41_r0.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
